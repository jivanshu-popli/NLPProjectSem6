{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrcQ-v2S4JmK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bev54YivTAeb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki-GKD7QBHq7"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suxiL4g024JN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.trainer_callback import ProgressCallback\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LMDIRDc5hyu"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lP4fyF4BMYX"
      },
      "source": [
        "## Downlaod dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDZtQXI-KwxN"
      },
      "outputs": [],
      "source": [
        "class EnglishDatasetLoader:\n",
        "    MAIN_DIR_PATH = 'http://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2'\n",
        "\n",
        "    @staticmethod\n",
        "    def load_train(type:object, size:object)->pd.DataFrame:\n",
        "        \"\"\"Loads the training dataset from WDC website\n",
        "        Args:\n",
        "            type (object): dataset type: computers, cameras, watches, shoes, all\n",
        "            size (object): dataset size: small, medium, large, xlarge\n",
        "        Returns:\n",
        "            pd.DataFrame: training dataset\n",
        "        \"\"\"\n",
        "        p = Path(os.path.join('trainsets', f'{type}_train'))\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "        dataset_path = f'{p}/{type}_train_{size}.json.gz'\n",
        "        if not os.path.exists(dataset_path):\n",
        "            zip_path = f'{p}.zip'\n",
        "            url = f'{EnglishDatasetLoader.MAIN_DIR_PATH}/trainsets/{type}_train.zip'\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            open(zip_path, 'wb').write(r.content)\n",
        "            with ZipFile(zip_path, 'r') as zip:\n",
        "                zip.extractall(path=p)\n",
        "        \n",
        "        df = pd.read_json(dataset_path, compression='gzip', lines=True)\n",
        "        return df.reset_index()\n",
        "\n",
        "    @staticmethod\n",
        "    def load_test(type:object)->pd.DataFrame:\n",
        "        \"\"\"Loads the test dataset form repository\n",
        "        Args:\n",
        "            type (object): dataset type: computers, cameras, watches, shoes, all\n",
        "        Returns:\n",
        "            pd.DataFrame: test dataset\n",
        "        \"\"\"\n",
        "        path = f'{EnglishDatasetLoader.MAIN_DIR_PATH}/goldstandards/{type}_gs.json.gz'\n",
        "        df = pd.read_json(path, compression='gzip', lines=True)\n",
        "        return df.reset_index()\n",
        "\n",
        "\n",
        "class FeatureBuilder:\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def get_X(self, dataset):\n",
        "        X = '[CLS] ' + dataset[f'{self.columns[0]}_left']\n",
        "        for i in range(1, len(self.columns)):\n",
        "            X = X + ' [SEP] ' + dataset[f'{self.columns[i]}_left']\n",
        "        for i in range(len(self.columns)):\n",
        "            X = X + ' [SEP] ' + dataset[f'{self.columns[i]}_right']\n",
        "        X + ' [SEP]'\n",
        "        return X.to_list()\n",
        "\n",
        "    def get_y(self, dataset):\n",
        "        return dataset['label'].to_list()\n",
        "\n",
        "\n",
        "class TorchPreprocessedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.items = self.preprocessItems(encodings, labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.items[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def preprocessItems(self, encodings, labels):\n",
        "        items = []\n",
        "        for idx in range(len(labels)):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            items.append(item)\n",
        "        return items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HtRhlwfBQoc"
      },
      "source": [
        "## Model definition, Example of dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtRR6KHWlWHx"
      },
      "outputs": [],
      "source": [
        "model_name = 'bert-base-multilingual-uncased'\n",
        "dataset_type = 'cameras'\n",
        "dataset_size = 'small'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imo6zNKrlpNL"
      },
      "outputs": [],
      "source": [
        "train_df = EnglishDatasetLoader.load_train(dataset_type, dataset_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNXPzOdW3D7R"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, sep_token = '[SEP]', cls_token = '[CLS]')\n",
        "title_fb = FeatureBuilder(['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxM_4qLu28Ki"
      },
      "outputs": [],
      "source": [
        "train_df = EnglishDatasetLoader.load_train(dataset_type, dataset_size)\n",
        "X_train = title_fb.get_X(train_df)\n",
        "y_train = title_fb.get_y(train_df)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "train_encodings = tokenizer(X_train , return_tensors='pt',  truncation=True, padding=True)\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = TorchPreprocessedDataset(train_encodings, y_train)\n",
        "val_dataset = TorchPreprocessedDataset(val_encodings, y_val)\n",
        "title_fb = FeatureBuilder(['title'])\n",
        "del train_df, X_train, X_val, y_train, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WCsa_QuKwxQ"
      },
      "outputs": [],
      "source": [
        "test_df = EnglishDatasetLoader.load_test(dataset_type)\n",
        "X_test = title_fb.get_X(test_df)\n",
        "y_test = title_fb.get_y(test_df)\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True)\n",
        "test_dataset = TorchPreprocessedDataset(test_encodings, y_test)\n",
        "del test_df, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMDiLqoL3HP4"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          \n",
        "    num_train_epochs=5,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,   # batch size per device during training\n",
        "    per_device_eval_batch_size=64,    # batch size for evaluation\n",
        "    warmup_steps=500,                 # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                # strength of weight decay\n",
        "    logging_dir=logdir,               # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    disable_tqdm=False,\n",
        "    fp16=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='no',\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                                  num_labels=2,\n",
        "                                                                  output_attentions=False,\n",
        "                                                                  output_hidden_states=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USWplqrgBalk"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukod_DdYX9qZ"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtvHytZDubfh"
      },
      "source": [
        "## Get Embeddings of each offer from the BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0Of2nqBrGWP"
      },
      "outputs": [],
      "source": [
        "# get embeddings\n",
        "import torch as th\n",
        "def getPooledOutputs(model, encoded_dataset, batch_size = 32):\n",
        "  model.eval()\n",
        "\n",
        "  # pooled_outputs = []\n",
        "  pooled_outputs = torch.empty([0,768]).cuda()\n",
        "  print(\"total number of iters \", len(encoded_dataset['input_ids'])//batch_size + 1)\n",
        "  \n",
        "  for i in range(len(encoded_dataset['input_ids'])//batch_size + 1):\n",
        "    print(i)\n",
        "    up_to = i*batch_size + batch_size\n",
        "    if len(encoded_dataset['input_ids']) < up_to:\n",
        "      up_to = len(encoded_dataset['input_ids'])\n",
        "    input_ids = th.LongTensor(encoded_dataset['input_ids'][i*batch_size:up_to]).cuda()\n",
        "    attention_mask = th.LongTensor(encoded_dataset['attention_mask'][i*batch_size:up_to]).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      embeddings = model.forward(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)['hidden_states'][-1][:,0] # Pooled output\n",
        "      pooled_outputs = th.cat([pooled_outputs, embeddings],0)\n",
        "      th.cuda.empty_cache()\n",
        "\n",
        "  return pooled_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh4IxRWm2eB6"
      },
      "outputs": [],
      "source": [
        "train_df = EnglishDatasetLoader.load_train(dataset_type, dataset_size)\n",
        "X_train = title_fb.get_X(train_df)\n",
        "y_train = title_fb.get_y(train_df)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "del X_val, y_train, y_val, train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZk3dhIszlPH"
      },
      "outputs": [],
      "source": [
        "def get_embedding_one_input(model,tokenizer, sentence):\n",
        "  \n",
        "  model.eval()\n",
        "  tok = tokenizer(sentence, return_tensors='pt')\n",
        "  input_ids = tok.input_ids.cuda()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    embeddings = model.forward(input_ids=input_ids, output_hidden_states=True)\n",
        "        \n",
        "  emb = embeddings.hidden_states[-1].cuda()\n",
        "\n",
        "  sep_idx = np.argwhere(np.array(sentence.split(\" \"))== '[SEP]')[0][0]\n",
        "\n",
        "  embedding1 = torch.empty([0,768]).cuda()\n",
        "  embedding2 = torch.empty([0,768]).cuda()\n",
        "  \n",
        "  for i in range(2, len(emb[0])-1):\n",
        "    if tok.word_ids()[i] < sep_idx:\n",
        "      embedding1 = torch.cat((embedding1, emb[:,i,:]), 0)\n",
        "\n",
        "    elif tok.word_ids()[i] > sep_idx:\n",
        "      embedding2 = torch.cat((embedding2, emb[:,i,:]), 0)\n",
        "\n",
        "  return embedding1, embedding2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyTaHXC3Bdyn"
      },
      "source": [
        "## Calculate cosine similarity between offer embedings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFj2IMg75plN"
      },
      "outputs": [],
      "source": [
        "def calculate_emb_cosine_metric(e1, e2):\n",
        "  avg1 = e1.mean(axis=0)\n",
        "  avg2 = e2.mean(axis=0)\n",
        "  return torch.cosine_similarity(avg1.reshape(1,-1), avg2.reshape(1,-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "469XT94luqwx"
      },
      "source": [
        "## Exemple for input sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB8cZ7AH5RJR"
      },
      "outputs": [],
      "source": [
        "sentence = X_train[1000]\n",
        "e1, e2 = get_embedding_one_input(model,tokenizer, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oemmBW45cq4"
      },
      "outputs": [],
      "source": [
        "calculate_emb_cosine_metric(e1, e2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF3Gzqlr_Rzs"
      },
      "outputs": [],
      "source": [
        "e1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1_r44LsA07U"
      },
      "outputs": [],
      "source": [
        "e2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b90b5480b90dfd82255d68efb607ef96370ef33575f247c89a0b81cbaa1e7b55"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
